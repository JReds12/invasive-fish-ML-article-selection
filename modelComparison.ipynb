{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e34751d",
   "metadata": {},
   "source": [
    "# Article Selection Model Comparison: Logistic Regression vs DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6f7e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\redinger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\redinger\\Documents\\Jupyter Projects\\articleSelect\\venvComparison\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "from transformers import DistilBertTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d332a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv files\n",
    "file_path = 'hypopthalmichthys_selected_articles.csv'\n",
    "carp_file = pd.read_csv(file_path, dtype = str)\n",
    "\n",
    "# Create new encoding for category column\n",
    "label_encoder = LabelEncoder()\n",
    "carp_file['encoding'] = label_encoder.fit_transform(carp_file['categories'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe824806",
   "metadata": {},
   "source": [
    "#### Data Preparation & Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f747fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy\n",
    "carp_clean = carp_file.copy()\n",
    "\n",
    "# Create new encoding for category column\n",
    "label_encoder = LabelEncoder()\n",
    "carp_clean['encoding'] = label_encoder.fit_transform(carp_clean['categories'])\n",
    "\n",
    "# Balance encoding categories\n",
    "def balance_df(df):\n",
    "\n",
    "    # Count the number of rows in each category\n",
    "    category_counts = df['encoding'].value_counts()\n",
    "\n",
    "    # Find the minority category\n",
    "    minority_category = category_counts.idxmin()\n",
    "\n",
    "    # Get the size of the minority category\n",
    "    minority_category_size = category_counts[minority_category]\n",
    "\n",
    "    # Sample rows from the majority category to match the size of the minority category\n",
    "    majority_category_rows = df[df['encoding'] != minority_category]\n",
    "    balanced_majority_category_rows = majority_category_rows.sample(n=minority_category_size, random_state=42)\n",
    "\n",
    "    # Get the minority category rows\n",
    "    minority_category_rows = df[df['encoding'] == minority_category]\n",
    "\n",
    "    # Concatenate the minority and balanced majority category rows\n",
    "    balanced_df = pd.concat([minority_category_rows, balanced_majority_category_rows])\n",
    "\n",
    "    return(balanced_df)\n",
    "    \n",
    "\n",
    "# Apply function\n",
    "carp_clean = balance_df(carp_clean)\n",
    "\n",
    "# Concatenate Title and Abstract\n",
    "carp_clean[\"TitleAbstract\"] = carp_clean[\"Title\"] + ' ' + carp_clean[\"Abstract\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc31c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create lemmenizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Filter out stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    # Reconstruct the text without stopwords\n",
    "    text_without_stopwords = ' '.join(filtered_tokens)\n",
    "    return text_without_stopwords\n",
    "\n",
    "    \n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "    \n",
    "carp_clean['Title'] = carp_clean['Title'].apply(clean_text)\n",
    "carp_clean['Abstract'] = carp_clean['Abstract'].apply(clean_text)\n",
    "carp_clean['TitleAbstract'] = carp_clean['TitleAbstract'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658c10e",
   "metadata": {},
   "source": [
    "#### Train-Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848acefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train_title, X_test_title, y_train, y_test = train_test_split(carp_clean['Title'], carp_clean['encoding'], test_size=0.2, random_state=42)\n",
    "X_train_abstract, X_test_abstract, _, _ = train_test_split(carp_clean['Abstract'], carp_clean['encoding'], test_size=0.2, random_state=42)\n",
    "X_train_combined, X_test_combined, _, _ = train_test_split(carp_clean['TitleAbstract'], carp_clean['encoding'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666f9fa",
   "metadata": {},
   "source": [
    "#### Tokenization for DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3cb4763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\redinger\\Documents\\Jupyter Projects\\articleSelect\\venvComparison\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\redinger\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\redinger\\Documents\\Jupyter Projects\\articleSelect\\venvComparison\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42433f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510    Extraction property acidsoluble collagen pepsi...\n",
       "22     Histological study sex differentiation bighead...\n",
       "571    facilitation native bluegill sunfish invasive ...\n",
       "46     Filterfeeding fish Hypophthalmichthys molitrix...\n",
       "153    Demographic rate variability bighead silver ca...\n",
       "                             ...                        \n",
       "106    Nonnative silver carp fail generalize behavior...\n",
       "547    Acquirement hrp conjunct igg antiigms widely c...\n",
       "531    Novel colorimetric film based starchpolyvinyl ...\n",
       "416    Lengthweight relationship five native fish spe...\n",
       "102    Identification micrornas silver carp Hypophtha...\n",
       "Name: Title, Length: 372, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd60aa58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_title = X_train_title.to_frame(name='text')  # Replace 'text' with the appropriate column name\n",
    "X_test_title = X_test_title.to_frame(name='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090b7cec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert datasets to tokenized format\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_title_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_title\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_title_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(X_test_title\u001b[38;5;241m.\u001b[39mto_frame)\n",
      "File \u001b[1;32m~\\Documents\\Jupyter Projects\\articleSelect\\venvComparison\\Lib\\site-packages\\datasets\\arrow_dataset.py:870\u001b[0m, in \u001b[0;36mDataset.from_pandas\u001b[1;34m(cls, df, features, info, split, preserve_index)\u001b[0m\n\u001b[0;32m    868\u001b[0m     info \u001b[38;5;241m=\u001b[39m DatasetInfo()\n\u001b[0;32m    869\u001b[0m info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m--> 870\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# more expensive cast than InMemoryTable.from_pandas(..., schema=features.arrow_schema)\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;66;03m# needed to support the str to Audio conversion for instance\u001b[39;00m\n\u001b[0;32m    877\u001b[0m     table \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mcast(features\u001b[38;5;241m.\u001b[39marrow_schema)\n",
      "File \u001b[1;32m~\\Documents\\Jupyter Projects\\articleSelect\\venvComparison\\Lib\\site-packages\\datasets\\table.py:720\u001b[0m, in \u001b[0;36mInMemoryTable.from_pandas\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pandas\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    666\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;124;03m    Convert pandas.DataFrame to an Arrow Table.\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Documents\\Jupyter Projects\\articleSelect\\venvComparison\\Lib\\site-packages\\pyarrow\\table.pxi:4525\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\Jupyter Projects\\articleSelect\\venvComparison\\Lib\\site-packages\\pyarrow\\pandas_compat.py:570\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataframe_to_arrays\u001b[39m(df, schema, preserve_index, nthreads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    563\u001b[0m                         safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    564\u001b[0m     (all_names,\n\u001b[0;32m    565\u001b[0m      column_names,\n\u001b[0;32m    566\u001b[0m      index_column_names,\n\u001b[0;32m    567\u001b[0m      index_descriptors,\n\u001b[0;32m    568\u001b[0m      index_columns,\n\u001b[0;32m    569\u001b[0m      columns_to_convert,\n\u001b[1;32m--> 570\u001b[0m      convert_fields) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_columns_to_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# using a thread pool is worth it. Currently the heuristic is whether the\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# nrows > 100 * ncols and ncols > 1.\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\Jupyter Projects\\articleSelect\\venvComparison\\Lib\\site-packages\\pyarrow\\pandas_compat.py:349\u001b[0m, in \u001b[0;36m_get_columns_to_convert\u001b[1;34m(df, schema, preserve_index, columns)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_columns_to_convert\u001b[39m(df, schema, preserve_index, columns):\n\u001b[1;32m--> 349\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_columns_of_interest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    353\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuplicate column names found: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m    354\u001b[0m         )\n",
      "File \u001b[1;32m~\\Documents\\Jupyter Projects\\articleSelect\\venvComparison\\Lib\\site-packages\\pyarrow\\pandas_compat.py:523\u001b[0m, in \u001b[0;36m_resolve_columns_of_interest\u001b[1;34m(df, schema, columns)\u001b[0m\n\u001b[0;32m    521\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m columns\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Convert datasets to tokenized format\n",
    "train_title_dataset = Dataset.from_pandas(X_train_title.to_frame)\n",
    "test_title_dataset = Dataset.from_pandas(X_test_title.to_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(examples):\n",
    "    return tokenizer(examples[\"cleaned_text\"], truncation=True)\n",
    "\n",
    "tokenized_title_train = train_title_dataset.map(tokenize_data, batched=True)\n",
    "tokenized_title_test = test_title_dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d303b88",
   "metadata": {},
   "source": [
    "#### Training DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Prepare data collator for padding sequences\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    learning_rate = 2e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 5,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\"\n",
    ")\n",
    "\n",
    "# Define Trainer object for training the model\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_title_train,\n",
    "    eval_dataset = tokenized_title_test,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29c4af",
   "metadata": {},
   "source": [
    "#### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4365a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predictions and true labels\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Calculate detailed metrics\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "\n",
    "# Print detailed metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8f65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvComparison",
   "language": "python",
   "name": "venvcomparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
