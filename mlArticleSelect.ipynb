{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b000b84d-051e-4dc2-8cd8-e645eb0d45c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\redinger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e585f-fb46-4509-8b85-b82549ef6ee5",
   "metadata": {},
   "source": [
    "#### Date Cleaning:\n",
    "Selected articles were indicated, but irrelevant articles were not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ec3c03-81d6-4b55-bd33-d9ab57a4ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "444\n",
      "110\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "# Create new column names because text file has no header\n",
    "col_names = [\"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Issue\", \"Pages\", \"URL\", \"Keywords\", \"Abstract\", \"DOI\", \"PDF Name\"]\n",
    "\n",
    "# Convert text files into a dataframe\n",
    "silver_yes = pd.read_csv('Hypopthalmichthys_molitrix_yes.txt', sep = '\\t', header = None, dtype = str, names = col_names, quotechar = '\"')\n",
    "silver_all = pd.read_csv('Hypopthalmichthys_molitrix_all.txt', sep = '\\t', header = None, dtype = str, names = col_names, quotechar = '\"')\n",
    "bighead_yes = pd.read_csv('Hypopthalmichthys_nobilis_yes.txt', sep = '\\t', header = None, dtype = str, names = col_names, quotechar = '\"')\n",
    "bighead_all = pd.read_csv('Hypopthalmichthys_nobilis_all.txt', sep = '\\t', header = None, dtype = str, names = col_names, quotechar = '\"')\n",
    "\n",
    "# Check entries of selected articles\n",
    "print(len(silver_yes))\n",
    "print(len(silver_all))\n",
    "print(len(bighead_yes))\n",
    "print(len(bighead_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3df702-5a24-40b8-8d6d-a64104c32559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n",
      "874\n"
     ]
    }
   ],
   "source": [
    "# Combine data with selected articles\n",
    "carp_yes = pd.concat([silver_yes, bighead_yes], ignore_index = True)\n",
    "\n",
    "# Combine all article - duplicates removed in next step to get irrelevant articles\n",
    "carp_concat = pd.concat([silver_yes, silver_all, bighead_yes, bighead_all], ignore_index = True)\n",
    "\n",
    "print(len(carp_yes))\n",
    "print(len(carp_concat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68c9b50-c055-4abf-a7d9-3137e4516ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates to get unselected articles\n",
    "carp_no = carp_concat.drop_duplicates(keep = False, ignore_index = True)\n",
    "\n",
    "print(len(carp_no))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3476984c-86a9-4d31-a91b-f8ca00d55c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset and select by columns\n",
    "columns = [\"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Issue\", \"Pages\", \"Abstract\"]\n",
    "\n",
    "carp_yes = carp_yes[columns]\n",
    "carp_no = carp_no[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d26d31-6e5b-4e67-a451-d1c86c8197f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add category columns\n",
    "carp_yes['categories'] = 'yes'\n",
    "carp_no['categories'] = 'no'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6757ae31-02a4-48d8-a5cc-ab6407a2fc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n"
     ]
    }
   ],
   "source": [
    "# combine df\n",
    "carp_all = pd.concat([carp_yes, carp_no], ignore_index = True)\n",
    "\n",
    "print(len(carp_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c26a5c9-8595-428c-976c-8d1dbe0173ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv files\n",
    "carp_all.to_csv('hypopthalmichthys_selected_articles.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d1264e-7ff5-42f8-a17e-bfe418af42eb",
   "metadata": {},
   "source": [
    "#### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71f5317b-028f-4c28-9075-bebf7f7b1053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n"
     ]
    }
   ],
   "source": [
    "# Import csv files\n",
    "carp_file = pd.read_csv('hypopthalmichthys_selected_articles.csv', dtype = str)\n",
    "\n",
    "print(len(carp_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "470c5fbc-237e-443a-a301-53a30c52ea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n"
     ]
    }
   ],
   "source": [
    "# Drop any abstracts with NAs\n",
    "carp_file.dropna(subset = ['Abstract'], inplace = True)\n",
    "\n",
    "print(len(carp_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e17c48df-fcbe-4d24-8ed8-8f45de3b74de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal Name</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aldridge, C. A., and E. C. Boone</td>\n",
       "      <td>2022</td>\n",
       "      <td>Simple models to quickly estimate the probable...</td>\n",
       "      <td>River Research and Applications</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>1154-1166</td>\n",
       "      <td>Species distribution models provide biologists...</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Banan, A., A. Nasiri, and A. Taheri-Garavand</td>\n",
       "      <td>2020</td>\n",
       "      <td>Deep learning-based appearance features extrac...</td>\n",
       "      <td>Aquacultural Engineering</td>\n",
       "      <td>89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fish species identification is vital for aquac...</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barnes, M. A., W. L. Chadderton, C. L. Jerde, ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Environmental conditions influence edna partic...</td>\n",
       "      <td>Environmental DNA</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>643-653</td>\n",
       "      <td>Knowledge about the size of environmental DNA ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Behera, B. K., A. K. Bera, P. Paria, A. Das, P...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Identification and pathogenicity of plesiomona...</td>\n",
       "      <td>Aquaculture</td>\n",
       "      <td>493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314-318</td>\n",
       "      <td>Plesiomonas shigelloides was isolated from dis...</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Borland, L. K., C. J. Mulcahy, B. A. Bennie, D...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Using markov chains to quantitatively assess m...</td>\n",
       "      <td>Natural Resource Modeling</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natural resource managers use barriers to dete...</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Author  Year  \\\n",
       "0                   Aldridge, C. A., and E. C. Boone  2022   \n",
       "1       Banan, A., A. Nasiri, and A. Taheri-Garavand  2020   \n",
       "2  Barnes, M. A., W. L. Chadderton, C. L. Jerde, ...  2021   \n",
       "3  Behera, B. K., A. K. Bera, P. Paria, A. Das, P...  2018   \n",
       "4  Borland, L. K., C. J. Mulcahy, B. A. Bennie, D...  2020   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Simple models to quickly estimate the probable...   \n",
       "1  Deep learning-based appearance features extrac...   \n",
       "2  Environmental conditions influence edna partic...   \n",
       "3  Identification and pathogenicity of plesiomona...   \n",
       "4  Using markov chains to quantitatively assess m...   \n",
       "\n",
       "                      Journal Name Volume Issue      Pages  \\\n",
       "0  River Research and Applications     38     6  1154-1166   \n",
       "1         Aquacultural Engineering     89   NaN        NaN   \n",
       "2                Environmental DNA      3     3    643-653   \n",
       "3                      Aquaculture    493   NaN    314-318   \n",
       "4        Natural Resource Modeling     33     4        NaN   \n",
       "\n",
       "                                            Abstract categories  encoding  \n",
       "0  Species distribution models provide biologists...        yes         1  \n",
       "1  Fish species identification is vital for aquac...        yes         1  \n",
       "2  Knowledge about the size of environmental DNA ...        yes         1  \n",
       "3  Plesiomonas shigelloides was isolated from dis...        yes         1  \n",
       "4  Natural resource managers use barriers to dete...        yes         1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new encoding for category column\n",
    "label_encoder = LabelEncoder()\n",
    "carp_file['encoding'] = label_encoder.fit_transform(carp_file['categories'])\n",
    "carp_file.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ad0b4-acc3-4f6c-8a7c-26866de93a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "965928d0-4c0e-4802-a5b6-a73e5e24e8fc",
   "metadata": {},
   "source": [
    "#### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2810f627-3407-4830-92ad-43c4685150ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of HTML tags and whitespace\n",
    "carp_file1 = carp_file\n",
    "\n",
    "# Function to remove HTML tags\n",
    "def basic_clean(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "    \n",
    "carp_file1['Title'] = carp_file1['Title'].apply(basic_clean)\n",
    "carp_file1['Abstract'] = carp_file1['Abstract'].apply(basic_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0c6b170-e922-4e17-a9f4-2d5e1176cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation, remove special characters, and remove stopwords\n",
    "carp_file2 = carp_file1\n",
    "\n",
    "# Get the set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean text\n",
    "def advanced_clean(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Function to remove stopwords from text\n",
    "    def remove_stopwords(text):\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # Filter out stopwords\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        # Reconstruct the text without stopwords\n",
    "        text_without_stopwords = ' '.join(filtered_tokens)\n",
    "        return text_without_stopwords\n",
    "\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "carp_file2['Title'] = carp_file2['Title'].apply(advanced_clean)\n",
    "carp_file2['Abstract'] = carp_file2['Abstract'].apply(advanced_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e36dfc24-d58e-4415-aa98-038dd606e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "carp_file3 = carp_file2\n",
    "\n",
    "stemmer = SnowballStemmer(language = 'english')\n",
    "def stem_words(text):\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "    \n",
    "carp_file3['Title'] = carp_file3['Title'].apply(stem_words)\n",
    "carp_file3['Abstract'] = carp_file3['Abstract'].apply(stem_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d9e1f5f-0272-451e-ac4f-b80a22606ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "carp_file4 = carp_file2\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "    \n",
    "carp_file4['Title'] = carp_file4['Title'].apply(stem_words)\n",
    "carp_file4['Abstract'] = carp_file4['Abstract'].apply(stem_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1852020f-4e75-4a90-b84a-711653742d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numerical values\n",
    "carp_file5 = carp_file2\n",
    "\n",
    "# Function to remove HTML tags\n",
    "def remove_digits(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "    \n",
    "carp_file5['Title'] = carp_file5['Title'].apply(remove_digits)\n",
    "carp_file5['Abstract'] = carp_file5['Abstract'].apply(remove_digits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvArticleSelect",
   "language": "python",
   "name": "venvarticleselect"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
